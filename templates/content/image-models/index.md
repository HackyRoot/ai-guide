<!-- TOC --><a name="image-generation-guide"></a>
# Image Generation Guide

The goal of this guide is to help the curious reader understand the technical foundations, applications, and challenges of image generation. 

This guide is neither a complete reference nor basic tutorial. It is meant to help you save time when getting started by providing an outline of the key ideas and linking high-quality reference material for further learning.

The initial sections are meant to be read in order, and serve as a gradual introduction to the topic. The final section ([Diagrams and Details](#diagrams-and-details)) contains summarized explanations of key ideas. These will likely be most useful _after_ the reader has become acquainted with the source material (linked in the [Recommended References](#recommended-references) section). 

<!-- TOC start (generated with https://github.com/derlin/bitdowntoc) -->

- [What is image generation?](#what-is-image-generation)
- [What are diffusion models?](#what-are-diffusion-models)
- [Challenges](#challenges)
   * [Selecting a base model](#selecting-a-base-model)
   * [Conditioning and Control](#conditioning-and-control)
   * [Consistency for video](#consistency-for-video)
   * [Performance](#performance)
- [Technical Foundations](#technical-foundations)
   * [Prerequisites](#prerequisites)
   * [Stable Diffusion and Control Nets](#stable-diffusion-and-control-nets)
   * [Stable Diffusion Inference Diagram](#stable-diffusion-inference-diagram)
- [Recommended References](#recommended-references)
- [Glossary](#glossary)
- [Diagrams and Details ](#diagrams-and-details)
   * [Diffusion Model](#diffusion-model)
   * [Autoencoder](#autoencoder)
   * [Latent Diffusion Model](#latent-diffusion-model)
   * [Variational Autoencoder (VAE)](#variational-autoencoder-vae)
   * [U-Net](#u-net)
   * [CLIP (Contrastive Language-Image Pretraining)](#clip-contrastive-language-image-pretraining)
   * [Text Encoder](#text-encoder)
   * [Control Net Conditioning Encoder](#control-net-conditioning-encoder)
   * [Control Net Trainable Copy](#control-net-trainable-copy)
   * [Low-Rank Adaptation (LoRA)](#low-rank-adaptation-lora)
   * [Dreambooth](#dreambooth)
   * [Textual Inversion](#textual-inversion)
   * [The U-Net Architecture (continued)](#the-u-net-architecture-continued)
      + [Overview](#overview)
      + [Key Concepts](#key-concepts)
   * [The Variational Autoencoder (VAE) (continued)](#the-variational-autoencoder-vae-continued)
      + [Overview](#overview-1)
      + [Key Concepts](#key-concepts-1)
   * [Noise, Timesteps, Schedulers](#noise-timesteps-schedulers)
      + [Overview](#overview-2)
      + [Key Concepts](#key-concepts-2)
   * [Text Encoder](#text-encoder-1)
      + [Overview](#overview-3)
      + [Key Concepts](#key-concepts-3)
   * [Fine-Tuning Techniques](#fine-tuning-techniques)
      + [Overview](#overview-4)
      + [Key Concepts](#key-concepts-4)

<!-- TOC end -->

<!-- TOC --><a name="what-is-image-generation"></a>
# What is image generation?

Image generation refers to the process of creating images using pre-trained models.

You've likely seen AI-generated images and [videos](https://www.youtube.com/watch?v=K10Ty0ZdbD8) online. Maybe you've generated your own images using applications like [`DALL-E`](https://openai.com/research/dall-e-3-system-card) or [`MidJourney`](https://www.midjourney.com/). This guide can help you dive deeper.

<figure>
<!-- <img src="/img/image-models/pope_fake.jpg"> -->
<img src="/img/image-models/screenshot_009.png">
<figcaption>This viral image of the pope wearing Balanciaga was generated by AI.</figcaption>
</figure>

Image generation is a wide and active area of research. This guide focuses on a subset of essential topics:

- **Forward and reverse diffusion**, the processes that enables today's popular _diffusion models_ to generates images,
- **Stable Diffusion**, a flexible, open-source diffusion model architecture, and
- **Challenges** in achieving a desired visual style and meeting performance requirements (e.g. speed, memory).

This guide does not attempt cover all topics. We omit or gloss over:
- Alternative neural network architectures for image generation like `GANs`
- Training details like hyperparameters, loss functions, and benchmarks
- Environment setup, getting started, and hardware considerations
- Neural Network fundamentals (e.g. stochastic gradient descent)

This guide does not attempt to be complete or exhaustive. Rather, we hope that the specific topics covered provide a solid foundation for further learning.

<!-- TOC --><a name="what-are-diffusion-models"></a>
# What are diffusion models?

[`Diffusion`](https://en.wikipedia.org/wiki/Diffusion) comes from the latin _diffundere_, meaning "to spread out". It is the familiar process of things moving from higher concentration to lower concentration. 

<img src="/img/image-models/screenshot_007.png">

When purple dye is dropped into water, the dye _diffuses_ until the entire mixture has a uniform purple hue. That is, until distribution of molecules from the purple dye is [uniformly random](https://en.wikipedia.org/wiki/Continuous_uniform_distribution).

<img src="/img/image-models/purple_diffusion.png">

`Diffusion models` are based on an analogous process. 

First, we collect images that look like what we want our model to generate. Each image is like a drop of "purple dye", rich in information and decidedly non-random. 

<img src="/img/image-models/screenshot_010.png">

Next, we add increasing levels of [noise](https://en.wikipedia.org/wiki/Gaussian_noise) to each image. We continue this diffusion process until the information has been "systematically destroyed". (That is, until the image has become uniform random noise.)

<figure>
<img src="/img/image-models/screenshot_011.png">
<figcaption>Forward-diffusion (noising)</figcaption>
</figure>

Noisy images are paired with their originals to form a training data set. We use this data set to train a model to recover the _original_ image given a noisy image. In other words, the model attempts to [remove the noise](https://en.wikipedia.org/wiki/Noise_reduction) we added. 

The model's loss function measures the difference between generated output and the original images, allowing us to improve the model parameters the usual way (stochastic gradient descent).

After training the model, we can use it to generate _new_ images: If we start with _pure noise_, the model will "denoise" it into something that _looks like_ it could be from the training set's original images.

<figure>
<img src="/img/image-models/screenshot_012.png">
<figcaption>Reverse-diffusion (denoising)</figcaption>
</figure>

We'll discuss diffusion models in more detail in the [Technical Foundations](#technical-foundations) section. First, let's consider some of the challenges we encounter when working with diffusion models. 

<!-- TOC --><a name="challenges"></a>
# Challenges

<!-- TOC --><a name="selecting-a-base-model"></a>
## Selecting a base model

If you are like most developers, you will not need to train diffusion models _from scratch_. Instead, you will select a pre-trained base model and either use it for inference directly or fine-tune it to your needs. (We'll mention several viable methods of fine-tuning in a later section.)

Like any AI model, generative image models reflect biases in their training data. It is important to choose models whose training data, terms of use, and degree of openness are consistent with your values and needs.

[Hugging Face](https://huggingface.co/models) hosts a large collection of models available for free download. Each model has a model card that describes its origin, capabilities, limitations and terms of use.

<figure>
<img src="/img/image-models/screenshot_001.png">
<figcaption>Stability AI's <a href="https://huggingface.co/stabilityai/sdxl-turbo">SDXL-Turbo</a> model card</figcaption>
</figure>
<!-- <figure> -->
<!-- <img src="/img/image-models/screenshot_004.png"> -->
<!-- <figcaption>Model cards often specify terms of use</figcaption> -->
<!-- </figure> -->
<figure>
<img src="/img/image-models/screenshot_005.png">
<figcaption>Model cards specify restrictions, limitations, and recommendations.</figcaption>
</figure>

[Civit AI](https://civitai.com/) offers a search tool for finding models that are already fine-tuned to produce a specific style.

<figure>
<img src="/img/image-models/screenshot_002.png">
<figcaption>Searching for a Pixel Art model on Civit AI</figcaption>
</figure>

<!-- TOC --><a name="conditioning-and-control"></a>
## Conditioning and Control

The most basic form of image generation is called _unconditional image generation_. Unconditional image generation means producing images that look like a random sample of the model's training data, with no additional input. If our training set contains pictures of cats, our model will generate pictures of cats. If our training set contains pictures of boats, our model will generate pictures of boats. 

The real power of diffusion models comes from our ability to _condition_ on various types of input data in order to tailor the output to our specific needs. When models make use of multiple types of data, we call them **multi-modal**. We'll consider a few examples.

[**CLIP**](https://arxiv.org/abs/2103.00020) and [**GLIDE**](https://arxiv.org/abs/2112.10741) allow models to be conditioned by text, enabling tasks like `Text-to-Image` and `Inpainting`.

<figure>
<img src="/img/image-models/screenshot_013.png">
<figcaption>Text-to-Image powered by GLIDE</figcaption>
</figure>

<figure>
<img src="/img/image-models/screenshot_014.png">
<figcaption>Inpainting powered by GLIDE</figcaption>
</figure>


Fine-tuning and transfer learning via techniques like [**Dreambooth**](https://arxiv.org/abs/2208.12242) and [**Textual Inversion**](https://arxiv.org/abs/2208.01618) allow a model to produce outputs of a specific subject or art style. 

<figure>
<img src="/img/image-models/screenshot_015.png">
<figcaption>A model trained to output a specific subject via Dreambooth</figcaption>
</figure>

In some cases, style-transfer can even be achieved _without_ fine-tuning, as shown by [**Style Aligned**](https://arxiv.org/abs/2312.02133)

<figure>
<img src="/img/image-models/screenshot_016.png">
<figcaption>Style Aligned models can output consistent style without requiring fine-tuning</figcaption>
</figure>

[**Control Nets**](https://arxiv.org/abs/2302.05543) are complementary models that, once trained, allow the output of a diffusion model can be _conditioned_ on (i.e. controlled by) skeletal animation poses, depth maps, segmentation maps, canny edges, and more. `Control Nets` showed how to go beyond text conditioning and opened up a huge number of previously infeasible use-cases for image generation.

<figure>
<img src="/img/image-models/screenshot_017.png">
<figcaption>Control Nets augment pre-trained models to allow fine-grained control</figcaption>
</figure>

<!-- TOC --><a name="consistency-for-video"></a>
## Consistency for video

We use Control Nets and style transfer so that we can generate images with consistent style and form, but the challenge of consistency goes beyond these techniques. Image-to-video models must additionally achieving consistency across video frames.

<figure>
<img src="/img/image-models/stable-video-diffusion.gif">
<figcaption><a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid">Stable Video Diffusion</a> generates videos from an input image.</figcaption>
</figure>

At the time of this writing, models are still in the early days of being trained to understand and generate video, with companies like RunwayML exploring what they're calling [General World Models](https://research.runwayml.com/introducing-general-world-models).

<!-- TOC --><a name="performance"></a>
## Performance

Image generation requires significant compute and memory resources. Since entire models are typically loaded into GPU memory, you will need to consider the capabilities and limitations of the systems on which your models run, so as to not exceed them.

<figure>
<img src="/img/image-models/screenshot_006.png">
<figcaption>A CUDA out-of-memory error</figcaption>
</figure>

Depending on the use case, it may be best prioritize inference speed over image quality, or vice versa.

If a model is too large to fit into GPU memory, **quantization** allows us to lower the memory requirements: If a model made up of 32-bit floats is too large, you can convert all the 32-bit floats to 16-bit floats, 8-bit floats, or even 4-bit floats. Downsizing floats reduces precision, but may allow models to run on less powerful hardward. Quantization of `float32` models to `float16` reduces memory requirements considerably without appreciable reduction in image quality.

[Tom ("TheBloke") Jobbins](https://huggingface.co/TheBloke) has released over 3000 models on Hugging Face. He uploads variants of popular models: quantized, fine-tuned, and with different file formats. He helps developers onboard into AI and run models on a wide variety of hardware. In August 2023, [a16z awarded Tom a grant](https://a16z.com/supporting-the-open-source-ai-community/) for his contributions to open source. 

Performance considerations affect everything from training, inference, and offering cloud services to end users. Here are a few references from Hugging Face:
- The [accelerate](https://huggingface.co/docs/accelerate/index) python package simplifies running PyTorch code across multiple devices (a "distributed configuration").
- These jupyter notebooks discuss some techniques / considerations for speeding up inference of diffusion models: [Accelerate inference](https://huggingface.co/docs/diffusers/tutorials/fast_diffusion#accelerate-inference-of-texttoimage-diffusion-models), [Effective and efficient diffusion](https://huggingface.co/docs/diffusers/stable_diffusion#effective-and-efficient-diffusion)
- This section of the `diffusers` docs considers many aspects of optimization: [Optimization Overview](https://huggingface.co/docs/diffusers/optimization/opt_overview)

<!-- TOC --><a name="technical-foundations"></a>
# Technical Foundations

There are many ways to generate images, which can make it difficult to learn the technical details. Different researchers and applications come up with their own unique implementations and architectures, making several important decisions along the way.

It helps to learn a few _specific_ end-to-end examples, rather than trying to learn general principles. This way, you can create a solid foundation in your mind and iteratively learn the variations and decisions that each implementation introduces.

We will focus on `Stable Diffusion`, a popular, open-source, and extensible model for image generation. We will also cover `Control Nets`, the flexible way of adding various conditioning to a stable diffusion model's outputs.

<!-- TOC --><a name="prerequisites"></a>
## Prerequisites

Before diving into Stable Diffusion, let's cover a few important aspects of all neural networks.

- **Inputs, Outputs, Loss, Parameters, and Layers/Blocks**: These are the building blocks of any neural network models. **Inputs** are tensors we feed into the model. **Outputs** are what the model produces. **Loss** is a measure used (during training) to quantify how far the model outputs differ from the desired output. Loss is used to update a model's **parameters**. Parameters are arranged into **layers**, and layers into **blocks**. Each layer and block serves some purpose within the architecture of the neural net. 

- **Training, Inference, and Fine-tuning**: These are the three main phases in the lifecycle of a neural network. **Training** involves learning from data to create a "base" or "foundation" model. **Inference** is using the model to make predictions (i.e. to feed some input to the model and let it generate an output). **Fine-tuning** is an optional training step that adjusts a pre-trained model to specific tasks or datasets.

- **Model Architectures**: A model _architecture_ refers to the particular configuration of layers, blocks, or even whole neural nets within a larger system. Different architectures excel at different tasks. We'll discuss a few types of models that are used in a Stable Diffusion pipeline.

<!-- TOC --><a name="stable-diffusion-and-control-nets"></a>
## Stable Diffusion and Control Nets

Stable Diffusion and Control Net have several components that work together to generate images. Architecture diagrams found in these systems' research papers assume the reader already understands the basic components involved. For example, consider the following diagrams from the Stable Diffusion and Control Net papers, respectively.

<figure>
<img src="/img/image-models/screenshot_019.png">
<figcaption>The architecture of Latent Diffusion Models, with conditioning</figcaption>
</figure>

<figure>
<img src="/img/image-models/screenshot_018.png">
<figcaption>How Control Nets augment Stable Diffusion's U-Net</figcaption>
</figure>

Our goal is to eventually understand these diagrams. 

We will start with a simplified diagram showing the entire architecture. Then we will examine the individual components until we know each component does and how it is trained. 

By the end of the article, we hope to understand the diagrams (and text) of the original research papers.

<!-- TOC --><a name="stable-diffusion-inference-diagram"></a>
## Stable Diffusion Inference Diagram

Let's examine the entire Stable Diffusion pipeline, including text conditioning and a Control Net trained on Canny edge maps.

<figure>
<img src="/img/image-models/screenshot_020.png">
<figcaption>Images generated by Stable Diffusion with a Control Net trained on Canny edge maps, further conditioned by text prompts.</figcaption>
</figure>

The following diagram illustrates the data flow in Stable Diffusion with a Control Net. Refer to the glossary and followup sections for more details about each component.

<figure>
<img src="/img/image-models/stable_diffusion_inference_diagram.png">
<figcaption>Data flow diagram of Stable Diffusion with Control Net integration.</figcaption>
</figure>

**Inputs and Outputs**
- **Input**: Text input and conditional input (Canny edge map).
- **Output**: Final generated image post reverse diffusion.

**Process Overview**
1. **Noisy Latent Initiation**: The process begins with a latent generated from pure noise.
2. **Text and Conditional Inputs Processing**: Text input and a Canny edge map are processed by their respective encoders to produce latent representations.
3. **Combining Latents**: The noise latent, text latent, image latent, and positional encoding (indicating the initial timestep) are combined into a comprehensive "noisy latent". This combination serves as a comprehensive input to the U-Net.
4. **U-Net Encoding and Decoding**: The combined latents pass through the U-Net encoder, which includes Control Net's trainable copy. The encoder's residual connections and Control Net's zero convolutions feed into the U-Net decoder, which predicts the latent noise, conditioned on the various inputs.
5. **Iterative Reverse Diffusion**: A scheduler adjusts the predicted noise - subtracting a portion of it and adding new noise to the latent. The updated noisy latent, now recombined, re-enters the U-Net for further processing. This loop continues, incrementing the timestep and updating positional encoding each iteration, until a predefined number of steps is reached.
6. **Final Image Generation with VAE Decoder**: The process ends with the VAE decoder transforming the denoised latent into the final image. Notice that the VAE encoder is not required for this (noise-to-image) task, though it was needed during training and would be needed for image-to-image tasks.

At this point, in order to deepend your understanding of the pipeline, I recommend reviewing the [recommended references](#recommended-references) (if you haven't already). The remainder of this guide consists of a [glossary](#glossary) defining key terms and summaries of the key concepts (in ["Diagrams and Details"](#diagrams-and-details)).

<!-- TOC --><a name="recommended-references"></a>
# Recommended References

These resources provide a deeper understanding of the concepts discussed. Of the many resources online, the author found these to be the most helpful and illuminating.

- Diffusion Models: [Research Paper](https://arxiv.org/abs/1503.03585)
- Latent Diffusion Models: [Research Paper](https://arxiv.org/abs/2112.10752)
- U-Net: [Research Paper](https://arxiv.org/abs/1505.04597), [Explainer Video](https://www.youtube.com/watch?v=NhdzGfB1q74)
- Convolution: [Grant Sanderson's video "But what is a convolution?"](https://www.youtube.com/watch?v=KuXjwB4LzSA)
- CLIP: [Website](https://openai.com/research/clip), [Research Paper](https://arxiv.org/abs/2103.00020)
- Variational Auto-Encoder: [Research Paper](https://arxiv.org/abs/1312.6114), [Explainer Video](https://www.youtube.com/watch?v=9zKuYvjFFS8)
- Stable Diffusion: [Explainer Video](https://www.youtube.com/watch?v=sFztPP9qPRc)
- ResNets: [Explainer Video 1](https://www.youtube.com/watch?v=o_3mboe1jYI), [Explainer Video 2](https://www.youtube.com/watch?v=Q1JCrG1bJ-A)
- Low-Rank Adaptations: [Research Paper](https://arxiv.org/abs/2106.09685)
- Control Nets: [Research Paper](https://arxiv.org/abs/2302.05543), [Paper Readthrough Video](https://www.youtube.com/watch?v=Mp-HMQcB_M4), [Note on zero-convolutions](https://github.com/lllyasviel/ControlNet/blob/ed85cd1e25a5ed592f7d8178495b4483de0331bf/docs/faq.md)
- History of diffusion models: [deepsense.ai blog post](https://deepsense.ai/the-recent-rise-of-diffusion-based-models/)

<!-- TOC --><a name="glossary"></a>
# Glossary

It will help to collect definitions several key terms, specifically in the context of their use in Stable Diffusion.

| Term                                 | Definition                                                                                                                                                                       |
|--------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Diffusion Model**                  | A model trained to generate images via reverse diffusion.                                                                                                                        |
| **Forward Diffusion**                | Gradually adding noise to an image, step by step, "systematically destroying" the information.                                                                                   |
| **Reverse Diffusion**                | Removing noise from an image, step by step, until a coherent image remains.                                                                                                      |
| **Conditioning**                     | Using alternative data sources (like text, canny-edges, etc) to alter the generated output of a diffusion model.                                                                 |
| **Autoencoder**                      | A model composed of a downsizing encoder and upsizing decoder. The space between the encoder and decoder is the latent space.                                                    |
| **Latent Space**                     | A low-dimensional space of tensors.                                                                                                                                              |
| **Pixel Space**                      | A high-dimensional space of (image) tensors (with shape `height x width x num_channels`).                                                                                        |
| **Latent Diffusion Model**           | A type of diffusion model that operates in a low-dimensional (and thus, more performant) latent space.                                                                           |
| **Stable Diffusion**                 | A set of implementations of latent diffusion models, like SD 1.4, SD 1.5, SDXL, and SDXL Turbo.                                                                                  |
| **Latents**                          | Tensors in latent space. High-dimensional data (images, text, canny-edges, etc) are converted to latents for processing.                                                         |
| **Variational Autoencoder**          | An autoencoder ensuring any random noise in latent space will decode to reasonable-looking images.                                                                               |
| **VAE Encoder**                      | Converts images to latents.                                                                                                                                                      |
| **VAE Decoder**                      | Converts latents to images.                                                                                                                                                      |
| **U-Net**                            | Core model of Stable Diffusion, receiving image latents and predicting their noise. Composed of an encoder and decoder, skilled at processing spatial hierarchies within images. |
| **U-Net Encoder**                    | The first half of the U-Net, downsizing images to capture semantic information.                                                                                                  |
| **U-Net Decoder**                    | The second half of the U-Net, upsizing images and recovering details lost by the encoder, augmented with semantic information.                                                   |
| **CLIP**                             | Contrastive language-image pretraining for creating text encoders whose embeddings are close to their associated image embeddings.                                               |
| **Text Encoder**                     | Converts text to latents, usually trained via CLIP.                                                                                                                              |
| **Control Net**                      | An architecture augmenting stable diffusion models for additional input conditioning.                                                                                            |
| **Control Net Conditioning Encoder** | Converts image conditioning data (like canny edge maps) into latent space for a control net.                                                                                     |
| **Control Net Trainable Copy**       | A trainable copy of the Stable Diffusion's U-Net Encoder, connected via "Zero Convolutions" for additional input conditioning without altering the main U-Net.                   |
| **Low-Rank Adaptation**              | A technique for efficiently fine-tuning large models by modifying only a small, low-rank subset of their parameters, enhancing model performance or adapting it to new tasks.    |
| **Dreambooth**                       | A specialized training procedure that personalizes generative models, like diffusion models, to generate content reflecting specific subjects or styles in the training data.    |
| **Textual Inversion**                | A process of adapting language models to understand and generate text or concepts not covered in their original training, by using specific, targeted training examples.         |


<!-- TOC --><a name="diagrams-and-details"></a>
# Diagrams and Details 

This section of the guide offers brief explanations and simplified diagrams (inputs and outputs) of the various components involved in a Stable Diffusion pipeline. 

It is recommended to consult the [recommended resources](#recommended-resources) for better explanations of each of these topics. This section should hopefully serve as a useful reminder and summary of the core concepts.

<!-- TOC --><a name="diffusion-model"></a>
## Diffusion Model

<figure>
<img src="/img/image-models/diffusion_model_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Randomly generated noise, (optionally) with non-random conditioning.

**Output**: A coherent generated image, produced from several iterative reverse diffusion steps.

**Training**: The training process for a diffusion model involves learning to denoise images. Once the model is able to reconstructing the original images from noisy inputs, we use it to generate new images from pure (or conditioned) noise.

<!-- TOC --><a name="autoencoder"></a>
## Autoencoder

<figure>
<img src="/img/image-models/autoencoder_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Original data that needs to be represented in a compressed form.

**Output**: Reconstructed data, which is the decompressed version of the data from the latent space.

**Description**: An autoencoder consists of two main parts: an encoder and a decoder. The encoder compresses the input data into a lower-dimensional latent space. The decoder then reconstructs the data from this latent space, aiming to produce an output as close as possible to the original input. Autoencoders are used for tasks like data denoising, dimensionality reduction, and feature learning.

<!-- TOC --><a name="latent-diffusion-model"></a>
## Latent Diffusion Model

<figure>
<img src="/img/image-models/latent_diffusion_model_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Latent noise, a lower-dimensional representation compared to full-resolution noise.

**Output**: A generated latent image, which is a lower-dimensional representation of the desired output image.

**Description**: The Latent Diffusion Model operates in a latent space, which is a compressed, lower-dimensional representation of the image data. This approach allows for more efficient processing and generation of images. The model learns to convert latent noise into a coherent latent image, which can then be transformed into a high-resolution image using other model components like decoders.

<!-- TOC --><a name="variational-autoencoder-vae"></a>
## Variational Autoencoder (VAE)

<figure>
<img src="/img/image-models/variational_autoencoder_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Original data for encoding into a latent representation.

**Output**: Reconstructed data, aimed to be as close to the original input as possible.

**Description**: A Variational Autoencoder is a type of autoencoder that introduces randomness in its encoding process. The encoder outputs a mean (μ) and variance (σ) for the latent representation. A random variable (ε) is then used to sample from this distribution, ensuring that the latent space can decode to a variety of similar but slightly different outputs. This characteristic makes VAEs particularly useful for generating new data that resembles the training data.

<!-- TOC --><a name="u-net"></a>
## U-Net

<figure>
<img src="/img/image-models/unet_model_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Image latents, which are lower-dimensional representations of images.

**Output**: Predicted image latents, mirroring the type and format of the input.

**Description**: The U-Net architecture is integral to the Stable Diffusion model. It consists of an encoder, which compresses the image latents, and a decoder, which reconstructs the latents back to their original form. The U-Net's structure enables it to efficiently process and understand spatial hierarchies within images. This makes it highly effective for tasks like image segmentation, denoising, and super-resolution.

<!-- TOC --><a name="clip-contrastive-language-image-pretraining"></a>
## CLIP (Contrastive Language-Image Pretraining)

<figure>
<img src="/img/image-models/clip_model_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Text and corresponding images.

**Output**: Text and image embeddings that are closely aligned in the embedding space.

**Description**: CLIP involves two encoders - one for text and one for images. The text encoder converts text inputs into embeddings, while the image encoder does the same for images. The core idea of CLIP is to train these encoders in a contrastive learning framework so that the embeddings of text and images that are semantically related are closer in the embedding space. This makes CLIP highly effective for tasks like zero-shot classification, where the model can recognize objects it hasn't explicitly been trained on.

<!-- TOC --><a name="text-encoder"></a>
## Text Encoder

<figure>
<img src="/img/image-models/text_encoder_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Raw text data.

**Output**: Latent representations of the text.

**Description**: A Text Encoder is a model that converts raw text into a latent, typically lower-dimensional, representation. These text latents are used in various applications, including natural language processing tasks and models like CLIP, where text embeddings need to be closely aligned with image embeddings for effective cross-modal learning. The text encoder is usually trained to capture the semantic essence of the input text, making it valuable for tasks like text classification, sentiment analysis, and more.

<!-- TOC --><a name="control-net-conditioning-encoder"></a>
## Control Net Conditioning Encoder

<figure>
<img src="/img/image-models/control_net_conditioning_encoder_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Image conditioning data, such as Canny edge maps or other processed image forms.

**Output**: Latent space representation of the conditioning data.

**Description**: The Control Net Conditioning Encoder is part of a control net system that processes additional image conditioning data into a latent representation. This encoder transforms specific forms of image data, like edge maps, into a format compatible with the model's latent space, enhancing the model's understanding and manipulation of image features. This component is crucial for models that require fine-grained control over image generation or modification.

<!-- TOC --><a name="control-net-trainable-copy"></a>
## Control Net Trainable Copy

<figure>
<img src="/img/image-models/control_net_trainable_copy_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Various additional inputs, including text and images.

**Output**: Enhanced image latents, incorporating additional input features.

**Description**: The Control Net Trainable Copy is a unique component in the Stable Diffusion architecture. It is a trainable copy of the U-Net Encoder, connected to the original Stable Diffusion's U-Net via Zero Convolutions. This setup allows the model to integrate additional inputs (like text or specific image features) into the generation process without altering the original, high-quality U-Net weights. This mechanism is key for adapting the model to new conditions or features while maintaining the integrity of the original model's capabilities.

<!-- TOC --><a name="low-rank-adaptation-lora"></a>
## Low-Rank Adaptation (LoRA)

<figure>
<img src="/img/image-models/low_rank_adaptation_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: Original parameters of a pre-trained model.

**Output**: Adapted model parameters, with updates focused on specific aspects.

**Description**: Low-Rank Adaptation (LoRA) is a technique for fine-tuning pre-trained models in a parameter-efficient manner. Instead of updating all parameters, LoRA focuses on adapting a small subset, typically through low-rank matrices. This approach allows for efficient and targeted modifications of the model, often used to adapt large models to specific tasks or datasets without the computational cost of full model retraining.

<!-- TOC --><a name="dreambooth"></a>
## Dreambooth

<figure>
<img src="/img/image-models/dreambooth_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: A set of target images and a specified reference class.

**Output**: A generative model fine-tuned to generate images that include characteristics of the target images.

**Description**: Dreambooth is a technique used to fine-tune a pretrained generative model so that it can generate images containing specific characteristics of a given target. This is done by training the model with a set of target images, along with a reference class that the targets belong to. The result is a model capable of creating new images that maintain the essence of the target images, allowing for personalized or targeted image generation.

<!-- TOC --><a name="textual-inversion"></a>
## Textual Inversion

<figure>
<img src="/img/image-models/textual_inversion_diagram.png">
<figcaption></figcaption>
</figure>

**Input**: A set of images depicting a specific concept and corresponding base text prompts.

**Output**: A language model adapted to understand and generate text related to the new, specific concept.

**Description**: Textual Inversion is a process where a pretrained language model is adapted to understand a new concept by training it with a set of images and associated text prompts. This method allows the model to incorporate a specific, often niche, concept into its understanding and generation capabilities. As a result, the model becomes more versatile in generating text that accurately reflects the new concept, enhancing its applicability to specialized or personalized tasks.

<!-- TOC --><a name="the-u-net-architecture-continued"></a>
## The U-Net Architecture (continued)

<!-- TOC --><a name="overview"></a>
### Overview
The U-Net architecture, originally designed for biomedical image segmentation, has become a cornerstone in image generation models. It is constructed from a series of downsampling an upsampling layers connected via skip connections, which allows it to process images with an awareness of spatial hierarchies. Its name comes from the "U" shape of its data flow diagram:

<img src="/img/image-models/screenshot_008.png">

<!-- TOC --><a name="key-concepts"></a>
### Key Concepts
- **Inputs and Outputs**: U-Nets take images as input and produce images (of the same height and width) as output. Their output might be segementation maps, canny edge maps, or whatever else they're trained to produce.
- **Convolution Layers**: These layers apply a convolution operation to the input, capturing local dependencies and learning features from the data. In [_But what is a convolution_](https://www.youtube.com/watch?v=KuXjwB4LzSA), Grant Sanderson provides an _excellent_ visual explanation of convolutions. 
- **Downsampling and Upsampling**: Downsampling reduces the spatial dimensions (height and width) of the image, helping the model to abstract and learn higher-level features. Upsampling, conversely, increases the spatial dimensions, aiding in reconstructing the finer details of the image. 
- **Residual Connections**: These connect layers of equal resolution in the downsampling and upsampling paths, allowing the model to preserve high-resolution features throughout the network. As with residual networks, these connections help alleviate the vanishing gradient problem, allowing deeper networks to be trained more effectively.

<!-- TOC --><a name="the-variational-autoencoder-vae-continued"></a>
## The Variational Autoencoder (VAE) (continued)

<!-- TOC --><a name="overview-1"></a>
### Overview
U-Nets are a great start, but training and inference on high-dimensional RGB images is too computationally expensive. We need a way to simplify.

Enter the auto-encoder. Autoencoders learn to encode data into a low-dimensional "latent space" and then decode from this space back to the original data. Unlike traditional _entropy-based_ compression, autoencoders' lossy compression algorithm is trained specifically for the data in their training set. 

An autoencoder is naturally broken into two halves: the encoder and the decoder. Once we have a trained autoencoder, we can use the encoder to compress high-resolution images into latent space and we can use the decoder to "decompress" tensors from the latent space back to high-resolution images.

We will embed a U-Net between the encoder and decoder of the VAE, so that the U-Net operates entirely in the latent space. (In other words, we do not directly feed high-resolution images to the U-Net during training or inference. We pre-process each image using the VAE's encoder.)

For our purposes, the "variational" part of VAEs is not a particularly important detail. _Variational_ auto-encoders ensure that the latent space of our auto-encoder has certain desireable properties, like having each dimension of the latent space correspond to "human-interpretable" changes to the images, and using fewer total dimensions when possible. 

<!-- TOC --><a name="key-concepts-1"></a>
### Key Concepts
- **Latent Space Representation**: VAEs compress input data into a lower-dimensional space, capturing the essence of the data in a more compact form.
- **Reconstruction Loss**: The VAE's loss function ensures that the decoded data matches the original input as closely as possible.
- **KL Divergence**: The VAE's loss function also regularizes the encoding space so that encodings follow a desired distribution (typically a Gaussian).

<!-- TOC --><a name="noise-timesteps-schedulers"></a>
## Noise, Timesteps, Schedulers

<!-- TOC --><a name="overview-2"></a>
### Overview
In diffusion models, noise plays a central role in transforming images. Timesteps and schedulers manage how noise is added and removed, influencing the model's efficiency and the quality of generated images.

<!-- TOC --><a name="key-concepts-2"></a>
### Key Concepts
- **Gaussian Noise**: This is the type of noise typically added to images in diffusion models. It follows a Gaussian (or normal) distribution.
- **Timesteps**: These represent discrete steps in the noise addition or removal process. Each timestep corresponds to a certain level of noise in the image.
- **Schedulers**: These algorithms determine the progression and pattern of timesteps, impacting the trade-off between image fidelity and computational requirements.

<!-- TOC --><a name="text-encoder-1"></a>
## Text Encoder

<!-- TOC --><a name="overview-3"></a>
### Overview
Text encoders in image generation models convert textual descriptions into a format that the model can use to guide the image generation process. 

<!-- TOC --><a name="key-concepts-3"></a>
### Key Concepts
- **Embeddings**: The text encoder converts text into a dense vector representation, capturing the semantic meaning in a form suitable for the model.
- **Conditioning**: The embeddings are used to condition the image generation process, aligning the output with the textual description.
- **CLIP**: A specific type of text encoder that aligns text and image representations, facilitating more accurate text-to-image generation.

<!-- TOC --><a name="fine-tuning-techniques"></a>
## Fine-Tuning Techniques

<!-- TOC --><a name="overview-4"></a>
### Overview
Fine-tuning involves adjusting a pre-trained model to better suit specific needs or styles. This can be done through various methods, each targeting different aspects of the model.

<!-- TOC --><a name="key-concepts-4"></a>
### Key Concepts
- **Dreambooth & Textual Inversion**: Techniques for personalizing models to generate images in a specific style or of a specific subject.
- **Low-Rank Adaptations**: Modifying a small subset of the model's parameters to fine-tune its outputs without the need for extensive retraining.

